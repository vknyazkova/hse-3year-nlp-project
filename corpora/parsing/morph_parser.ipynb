{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1ii_68jzjfTx",
        "iRWLvFkejpRk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Парсинг"
      ],
      "metadata": {
        "id": "1ii_68jzjfTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8wtjFzuHbizb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('corpus.json') as json_file:\n",
        "    dictionary = json.load(json_file)"
      ],
      "metadata": {
        "id": "E0RtLj03PLEn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "values = list(dictionary.values())"
      ],
      "metadata": {
        "id": "yjRSGB73LE5G"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(values)): # preprocess corpus if there are any excess symbols\n",
        "  values[i][0] = re.sub('\\\\n', '', values[i][0])\n",
        "  values[i][0] = re.sub('\\\\n\\\\n', '', values[i][0])\n",
        "  values[i][0] = re.sub('\\\\r\\\\n', '', values[i][0])"
      ],
      "metadata": {
        "id": "3Rg7MfNuKe6H"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy_conll import init_parser\n",
        "from spacy.symbols import ORTH\n",
        "import spacy\n",
        "\n",
        "nlp = init_parser(\"ru_core_news_sm\", 'spacy', include_headers=True)"
      ],
      "metadata": {
        "id": "MBfhbatWIhoY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "specials = ['мин.', 'ст. л.', 'ч. л.', '180°C']"
      ],
      "metadata": {
        "id": "44vB2J42qETg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link = 'https://www.gastronom.ru/recipe/'\n",
        "for i in dictionary:\n",
        "  current_link = f'{link}{i}'\n",
        "  doc = nlp(dictionary[i][0])\n",
        "  for spec in specials:\n",
        "    if spec[:2] in doc.text:\n",
        "      if spec[:-2] in doc.text:\n",
        "        nlp.tokenizer.add_special_case(spec, [{ORTH: spec}])\n",
        "  conll = doc._.conll_str\n",
        "  conll = re.sub('([0-9]+)\\t{5}SPACE\\tSPACE.+\\n', '', conll)\n",
        "  conll = re.sub('\\n\\n', f'\\n\\n# source = {current_link}\\n# name = {dictionary[i][1]}\\n', conll)\n",
        "  with open('corpus_parsed.conllu', 'a', encoding='utf-8') as f:\n",
        "    f.write(f'\\n# source = {current_link}\\n# name = {dictionary[i][1]}\\n')\n",
        "    f.write(conll)"
      ],
      "metadata": {
        "id": "10dU2zKoa_xI"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}